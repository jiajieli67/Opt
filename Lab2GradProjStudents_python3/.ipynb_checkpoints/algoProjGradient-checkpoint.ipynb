{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Projected Gradient-based algorithms\n",
    "\n",
    "In this notebook, we code our Projected gradient-based optimization algorithms.\n",
    "We consider here \n",
    "* Positivity constraints\n",
    "* Interval constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Projected Gradient algorithms (for positivity or interval constraints)\n",
    "\n",
    "For minimizing a differentiable function $f:\\mathbb{R}^n \\to \\mathbb{R}$, given:\n",
    "* the function to minimize `f`\n",
    "* a 1st order oracle `f_grad` (see `problem1.ipynb` for instance)\n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "\n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = P\\left(x_k - \\gamma_k \\nabla f(x_k)\\right) $$\n",
    "where $\\gamma_k$ is a stepsize to choose and $P$ is the projector onto the convex constraint set. We only consider positivity and interval constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Constant stepsize projected gradient algorithm for positivity constraints\n",
    "\n",
    "First, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm.\n",
    "\n",
    "> Fill the function below accordingly. Then, test you algorithm in `2_Optimization100.ipynb [Sec. 1a]` for Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def positivity_gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    xp = np.copy(x0)\n",
    "    g = f_grad(xp) # we initialize both x and f_grad(x)\n",
    "    #stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "    stop = PREC\n",
    "    \n",
    "    zero = np.zeros_like(x0) #could be usefull for convergence test...\n",
    "    \n",
    "    x_tab = np.copy(xp)\n",
    "    print(\"------------------------------------\\n Constant Stepsize projected gradient for positivity contraints\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        # g = f_grad(x)\n",
    "        g = f_grad(xp)\n",
    "        x = xp - step * g   \n",
    "        #######  ITERATION --> To complete by the projection onto the set \"x >= 0\"\n",
    "        for i in range(0,np.size(x)):\n",
    "            if (x[i]<0):\n",
    "                x[i] = 0\n",
    "        #######\n",
    "        \n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "        ####### \n",
    "        # Write the convergence test: which gradient component should participate ?\n",
    "        #######  Why must the following stopping criteria be changed ? Propose a correct stopping rule\n",
    "        if np.linalg.norm(x-xp) < stop:\n",
    "            break\n",
    "        xp = x\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f} at point ({:.2f},{:.2f})\\n\\n\".format(k,t_e-t_s,f(x),x[0],x[1]))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Constant stepsize projected gradient algorithm for interval constraints\n",
    "\n",
    "First, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm.\n",
    "\n",
    "> Fill the function below accordingly. Then, test you algorithm in `2_Optimization100.ipynb [Sec. 1a]` for Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def interval_gradient_algorithm(f , f_grad , x0 , infbound , supbound , step , PREC , ITE_MAX ):\n",
    "    # compute the min of f with a gradient method with constant step under the constraint \n",
    "    # infbound <= x <= supbound\n",
    "    x = np.copy(x0)\n",
    "    xp = np.copy(x0)\n",
    "    g = f_grad(xp)\n",
    "    stop = PREC\n",
    "\n",
    "    x_tab = np.copy(xp)\n",
    "    print(\"------------------------------------\\n Constant Stepsize projected gradient for interval constraints\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(xp)\n",
    "        x = x - step * g   \n",
    "        #######  ITERATION --> To complete by the projection onto the set \"x >= 0\"\n",
    "        for i in range(0,np.size(x)):\n",
    "            if (x[i]<infbound[i]):\n",
    "                x[i] = infbound[i]\n",
    "            elif(x[i]>supbound[i]):\n",
    "                x[i] = supbound[i]\n",
    "        ####### \n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "        ####### \n",
    "        #Write the convergence test: which gradient component should participate ?   \n",
    "        #######  Why must the following stopping criteria be changed ? Propose a correct stopping rule\n",
    "        if np.linalg.norm(x-xp) < stop:\n",
    "            break\n",
    "        xp = x\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f} at point ({:.2f},{:.2f})\\n\\n\".format(k,t_e-t_s,f(x),x[0],x[1]))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Adaptive stepsize gradient algorithm\n",
    "\n",
    "Now, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm.\n",
    "\n",
    "> Examine the behavior of the constant stepsize gradient algorithm in `2_Optimization201.ipynb [Sec. 2b]` for non-convex Problem 3 (Rosenbrock function) and try to solve the problem by changing the stepsizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interval_gradient_adaptive_algorithm(f , f_grad , x0 , infbound , supbound , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    g = f_grad(x)\n",
    "    stop = PREC*np.linalg.norm(g)\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\nAdaptative Stepsize gradient\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        \n",
    "        x_prev = np.copy(x)\n",
    "        \n",
    "        x = x - step*g  ## ITERATION -> To adapt in order to introduce interval constraints x in [infbound , supbound] \n",
    "        if f(x)>f(x_prev):\n",
    "            x = np.copy(x_prev)\n",
    "            step = step/2\n",
    "            print(\"stepsize: = {:0}\".format(step))\n",
    "            \n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "        \n",
    "        g = f_grad(x)\n",
    " \n",
    "       ####### \n",
    "        #Write the convergence test: which gradient component should participate ?   \n",
    "        #######  Why must the following stopping criteria be changed ? Propose a correct stopping rule\n",
    "        #if np.linalg.norm(g) < stop:\n",
    "        #    break\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f} at point ({:.2f},{:.2f})\\n\\n\".format(k,t_e-t_s,f(x),x[0],x[1]))\n",
    "    return x,x_tab"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
