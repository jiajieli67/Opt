{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 32\n",
    "\n",
    "\n",
    "The objective of Problem 31 is to minimize a simple quadratic function $f$ on $\\mathbb{R}^2$ under affine constraints: \n",
    "\n",
    "$$\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/1.png\" width=\"50%\"></center>\n",
    "\n",
    "The affine constraints are given on the form $\\theta_i \\cdot x \\le d_i, i=1,\\ldots, m$ where $\\theta_i$ is a vector. We start here with a unique constraint ($m=1$),  \n",
    "$\\theta_1=(1,-1)$ and $d_1=1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Function definition\n",
    "def f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return 4*(x1-3)**2+2*(x2-1)**2\n",
    "####\n",
    "\n",
    "##### Plot parameters f\n",
    "x1_min = -0.5\n",
    "x1_max = 5.5\n",
    "x2_min = -0.5\n",
    "x2_max = 5.5\n",
    "nb_points = 200\n",
    "vmin = 0\n",
    "vmax = 80\n",
    "levels = [0.5,1,2,5,10,15]\n",
    "title = 'f: a simple function'\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##### Constraint functions definition\n",
    "\n",
    "C = np.array([[1,-1],[2,-1]])\n",
    "d = np.array([1,3])\n",
    "##### Constraint functions definition\n",
    "def phi1(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    C11 = C[0][0]\n",
    "    C12 = C[0][1]\n",
    "    d1 = d[0]\n",
    "    return C11*x1 + C12*x2 - d1;\n",
    "    #return np.dot(C, x) - d;\n",
    "    \n",
    "def phi2(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    C21 = C[1][0]\n",
    "    C22 = C[1][1]\n",
    "    d2 = d[1]\n",
    "    return C21*x1 + C22*x2 - d2;\n",
    "    #return np.dot(C, x) - d;\n",
    "\n",
    "##### Plot parameters phi (as for f)\n",
    "phi1nb_points = 100\n",
    "phi1vmin = -10\n",
    "phi1vmax = 10\n",
    "phi1levels = [-3,-2,-1,0,1,2,3]\n",
    "phi1title = 'phi1: a simple affine constraint'\n",
    "####\n",
    "\n",
    "def phi(x):\n",
    "    p1 = phi1(x)\n",
    "    p2 = phi2(x)\n",
    "    return np.array( [ p1,p2 ] )\n",
    "####    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some parameters\n",
    "\n",
    "Before solving things numerically, some useful things can be computed:\n",
    "* Properties of $f$: lower bounds, Elliptic constant $\\alpha$ of $f$, Lipschitz constant of $\\nabla f$, strong convexity constant, norm of the matrix C of the affine constraints, etc\n",
    "* Good starting points (for hot starting e.g.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Constraints parameters   \n",
    "### -> To complete  \n",
    "alpha = 4\n",
    "normeCsquared = np.linalg.norm(C)**2\n",
    "\n",
    "rhomax = 2*alpha/normeCsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Useful Parameters\n",
    "L = 8        # Lipschitz constant of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracles\n",
    "\n",
    "Numerical optimization methods need callable *oracles* for properties of $f$, that is a function that, given a point $x$ in the domain of $f$, returns $f$ and/or gradient, Hessian of $f$ at point $x$. We talk about the *order* of an oracle as the number of differentiations given (0th order for just $f$, 1st order for the gradient, 2nd for gradient + Hessian).\n",
    "\n",
    "> Observe the first order oracle `f_grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##### Gradient oracle\n",
    "def f_grad(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    gx = 8*(x1-3) \n",
    "    gy = 4*(x2-1)\n",
    "    return np.array( gx  ,  gy   )\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the following second order oracle `f_grad_hessian`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##### Hessian scaled Gradient computation\n",
    "def f_grad_hessian(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    gx = 8*(x1-3) \n",
    "    gy = 4*(x2-1) \n",
    "    g = np.zeros(2)\n",
    "    g[0] = gx\n",
    "    g[1] = gy\n",
    "    H = np.zeros( (2,2)  )  ### -> To complete DONE \n",
    "    H[0][0] = 8\n",
    "    H[1][1] = 4\n",
    "    return g,H\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian oracles\n",
    "definition of the Lagrangian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Lagrangian definition\n",
    "def lagrangian(x,mu):\n",
    "    \n",
    "    L = f(x) + np.dot(mu,phi(x))\n",
    "    return L\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##### Lagrangian Hessian and Lagrangian Gradient computation of the Lagrangian \n",
    "def lagrangian_grad_hessian(x,mu):\n",
    "    \n",
    "    gJ, HJ = f_grad_hessian(x) \n",
    "    g = gJ + np.dot(np.transpose(C),mu)\n",
    "    \n",
    "    return g,HJ\n",
    "####"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
