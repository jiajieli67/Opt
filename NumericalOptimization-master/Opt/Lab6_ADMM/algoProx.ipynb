{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal algorithms\n",
    "\n",
    "In this notebook, we code our proximal optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Proximal Gradient algorithm\n",
    "\n",
    "For minimizing a function $F:\\mathbb{R}^n \\to \\mathbb{R}$ equal to $f+g$ where $f$ is differentiable and the $\\mathbf{prox}$ of $g$ is known, given:\n",
    "* the function to minimize `F`\n",
    "* a 1st order oracle for $f$ `f_grad` \n",
    "* a proximity operator for $g$ `g_prox` \n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "* a display boolean variable `PRINT` \n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = \\mathbf{prox}_{\\gamma g}\\left( x_k - \\gamma \\nabla f(x_k) \\right) $$\n",
    "where $\\gamma$ is a stepsize to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def proximal_gradient_algorithm(F , f_grad , g_prox , x0 , step , PREC , ITE_MAX , PRINT ):\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "    if PRINT:\n",
    "        print(\"------------------------------------\\n Proximal gradient algorithm\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(x)\n",
    "        x = g_prox(x - step*g , step)  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    if PRINT:\n",
    "        print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,F(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ADMM\n",
    "\n",
    "For minimizing a function $F:\\mathbb{R}^n \\to \\mathbb{R}$ equal to $f+g$ where the $prox$ of $f$ and $g$ are known, given:\n",
    "* the function to minimize `F`\n",
    "* a proximity operator for $f$ `f_prox` \n",
    "* a proximity operator for $g$ `g_prox` \n",
    "* a parameter $\\rho>0$ `rho` \n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "* a display boolean variable `PRINT` \n",
    "\n",
    "The ADMM perform iterations of the form\n",
    "\\begin{align*}\n",
    "x_{k+1} &= \\mathbf{prox}_{f/\\rho} \\left( z_k - \\lambda_k/\\rho \\right) \\\\\n",
    "z_{k+1} &= \\mathbf{prox}_{g/\\rho} \\left( x_{k+1} + \\lambda_k/\\rho \\right) \\\\\n",
    "\\lambda_{k+1} &= \\lambda_k + \\rho\\left( x_{k+1} - z_{k+1} \\right)\n",
    "\\end{align*}\n",
    "where $\\rho>0$ is an hyper-parameter to set. It is also greatly interesting to keep track of the *primal and dual residuals*:\n",
    "$$ p_k = x_k - z_k \\text{ [Primal residual]}  ~~~~ \\text{ and } ~~~~  d_k = \\rho(z_k - z_{k-1} )  \\text{[Primal residual]}$$\n",
    "\n",
    "The values return by the function should be:\n",
    "* the final $z$-point  `x`\n",
    "* the table of the iterates $(z_k)$ for all iterations `x_tab`\n",
    "* the list of the norms of the primal residuals $(\\|p_k\\|_2) $ for all iterations `p_tab`\n",
    "* the list of the norms of the dual residuals $(\\|d_k\\|_2) $ for all iterations `d_tab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Fill the function below with the ADMM.\n",
    "\n",
    "> Implement a precision stopping criteria. For Instance, in `Boyd et al. \"Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers\" Foundations and Trends in Machine Learning, 2011`, the adviced termination criterion is \n",
    "$$ \\|p_k\\|_2 \\leq \\varepsilon \\text{ and }  \\|d_k\\|_2 \\leq \\varepsilon . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def ADMM(F , f_prox , g_prox , rho , x0 , PREC , ITE_MAX , PRINT ):\n",
    "    x = np.copy(x0)\n",
    "    z = np.copy(x0)\n",
    "    lam = np.copy(x0)\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    p_tab = []\n",
    "    d_tab = []\n",
    "        \n",
    "    if PRINT:\n",
    "        print(\"------------------------------------\\n ADMM\\n------------------------------------\\nSTART    -- rho = {:0}\".format(rho))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "\n",
    "        ### UPDATE : TO COMPLETE\n",
    "        \n",
    "\n",
    "        x_tab = np.vstack((x_tab,z))\n",
    "\n",
    "        p = 1.0 #TODO\n",
    "        d = 1.0 #TODO\n",
    "        p_tab.append(float(p))\n",
    "        d_tab.append(float(d))\n",
    "\n",
    "        # STOPPING CRITERIA TO IMPLEMENT\n",
    "        \n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    if PRINT:\n",
    "        print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,F(x)))\n",
    "    return z,x_tab,p_tab,d_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
