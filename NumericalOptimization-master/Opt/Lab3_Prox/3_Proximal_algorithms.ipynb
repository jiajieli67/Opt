{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab on Proximal Algorithms</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Composite minimization for machine learning.\n",
    "\n",
    "In this lab, we will investigate optimization algorithms over composite functions composed of a smooth and a non-smooth part using the proximal gradient algorithm over a practical problem of machine learning: binary classification using logistic regression.</br>\n",
    "\n",
    "> Read the file `logistic_regression_student.ipynb` that contains the problem explanation and simulators. \n",
    "\n",
    "> Implement the proximal operation linked to $\\ell_1$ norm in `logistic_regression_student.ipynb`. \n",
    "\n",
    "> Implement the proximal gradient algorithm and test yout algorithm below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logistic_regression_student as pb\n",
    "reload(pb)\n",
    "\n",
    "\n",
    "##### proximal gradient algorithm\n",
    "#x,x_tab = proximal_gradient_algorithm(pb.F , pb.f_grad , pb.g_prox , x0 , step , PREC, ITE_MAX , True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the decrease of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the support of the vector $x_k$ (i.e. one point for every non-null coordinate of $x_k$) versus the iterations. \n",
    "\n",
    "> What do you notice? Was it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regularization path.\n",
    "\n",
    "\n",
    "We saw above that the algorithm *selected* some coordinates as the other get to zero. Considering our machine learning task (see `logistic_regression_student.ipynb`), this translates into the algorithm selecting a subset of the features that will be used for the prediction step.  \n",
    "\n",
    "> Change the parameter $\\lambda_1$ of the problem (`pb.lam1`) in the code above and investigate how it influences the number of selected features.\n",
    "\n",
    "In order to quantify the influence of this feature selection, let us consider the *regularization path* that is the support of the final points obtained by our minimization method versus the value of $\\lambda_1$.\n",
    "\n",
    "> For $\\lambda_1 = 2^{-12},2^{-11}, .. , 2^{1}$, run the proximal gradient algorithm on the obtained problem and store the support of the final point, the prediction performance on the *training set* (`pb.prediction_train`) and on the *testing set* (`pb.prediction_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_student as pb\n",
    "reload(pb)\n",
    "\n",
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *regularization path* and look at the feature signification (file `student.txt` or `logistic_regression_student.ipynb`) to see which are the most important features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *training* and *testing* accuracies versus the value of $\\lambda_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
