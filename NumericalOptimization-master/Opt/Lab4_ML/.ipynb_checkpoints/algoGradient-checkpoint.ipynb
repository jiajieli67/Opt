{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient-based algorithms\n",
    "\n",
    "In this notebook, we code our gradient-based optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Gradient algorithms\n",
    "\n",
    "For minimizing a differentiable function $f:\\mathbb{R}^n \\to \\mathbb{R}$, given:\n",
    "* the function to minimize `f`\n",
    "* a 1st order oracle `f_grad` (see `problem1.ipynb` for instance)\n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "\n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = x_k - \\gamma_k \\nabla f(x_k) $$\n",
    "where $\\gamma_k$ is a stepsize to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Constant stepsize gradient algorithm\n",
    "\n",
    "First, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm.\n",
    "\n",
    "> Fill the function below accordingly. Then, test you algorithm in `1_Optimization101.ipynb [Sec. 1a]` for Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n Constant Stepsize gradient\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(x)\n",
    "        x = x - step*g  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Adaptive stepsize gradient algorithm\n",
    "\n",
    "Now, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm.\n",
    "\n",
    "> Examine the behavior of the constant stepsize gradient algorithm in `1_Optimization101.ipynb [Sec. 2b]` for non-convex Problem 3 (Rosenbrock function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "def gradient_adaptive_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\nAdaptative Stepsize gradient\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        \n",
    "        g = f_grad(x)\n",
    "        x_prev = np.copy(x)\n",
    "        \n",
    "        x = x - step*g  #######  ITERATION\n",
    "\n",
    "        if f(x)>f(x_prev):\n",
    "            x = np.copy(x_prev)\n",
    "            step = step/2\n",
    "            print(\"stepsize: = {:0}\".format(step))\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Wolfe Line search\n",
    "\n",
    "\n",
    "> Complete the function below accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def gradient_Wolfe(f , f_grad , x0 , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n Gradient with Wolfe line search\\n------------------------------------\\nSTART\")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(x)\n",
    "        \n",
    "        res = line_search(f, f_grad, x, -g, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
    "        #print(res)\n",
    "        x = x - res[0]*g \n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d. Nesterov's Fast gradient algorithm\n",
    "\n",
    "In a series of papers published in the 80's, Yu. Nesterov proposed an acceleration technique in order to make the worst case rate of the gradient algorithm from $\\mathcal{O}(1/k)$ to  $\\mathcal{O}(1/k^2)$. This technique is now immensely popular, notably in the machine learning and image processing communities.\n",
    "\n",
    "The iterations of Nesterov's accelerated gradient are as such:\n",
    "$$ \\left\\{  \\begin{array}{ll}  x_{k+1} = y_k - \\gamma \\nabla f(y_k) \\\\ y_{k+1} = x_{k+1} + \\alpha_{k+1} (x_{k+1} - x_k )  \\end{array}           \\right. $$\n",
    "with \n",
    "$$ \\alpha_{k+1} = \\frac{\\lambda_k -1 }{\\lambda_{k+1}} \\text{ with } \\lambda_0 = 0 \\text{ and } \\lambda_{k+1} = \\frac{1+\\sqrt{1+4\\lambda_k^2}}{2} . $$\n",
    "\n",
    "Although no clear intuition can be drawn, the extended point can be seen as an extension by inertia of the last points.\n",
    "\n",
    "\n",
    "> Fill the function below accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def fast_gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    y = np.copy(x0)\n",
    "    lamda = 0\n",
    "    stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "    \n",
    "\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n Fast gradient\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(y)\n",
    "        x_kplusone = y - step*g\n",
    "        x_tab = np.vstack((x_tab,x_kplusone))\n",
    "        lamda_kplusone = (1+np.sqrt(1+4*lamda**2))/2\n",
    "        alpha = (lamda-1)/lamda_kplusone\n",
    "        y = x_kplusone +alpha*(x_kplusone - x)\n",
    "        x = x_kplusone\n",
    "        lamda = lamda_kplusone\n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Second Order algorithms\n",
    "\n",
    "For minimizing a *twice* differentiable function $f:\\mathbb{R}^n \\to \\mathbb{R}$, given:\n",
    "* the function to minimize `f`\n",
    "* a 2nd order oracle `f_grad_hessian` (see `problem1.ipynb` for instance)\n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "\n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = x_k - [\\nabla^2 f(x_k) ]^{-1} \\nabla f(x_k) .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def newton_algorithm(f , f_grad_hessian , x0 , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    g0,H0 = f_grad_hessian(x0)\n",
    "    stop = PREC*np.linalg.norm(g0 )\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\nNewton's algorithm\\n------------------------------------\\nSTART\")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "    \n",
    "        g,H = f_grad_hessian(x)\n",
    "        x = x - np.linalg.solve(H,g)  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "        \n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Quasi Newton algorithms\n",
    "\n",
    "**BFGS.** (Broyden-Fletcher-Goldfarb-Shanno, 1970) The popular BFGS algorithm consist in performing the following iteration\n",
    "$$ x_{k+1}=x_k - \\gamma_k W_k \\nabla f(x_k)$$\n",
    "where $\\gamma_k$ is given by Wolfe's line-search and positive definite matrix $W_k$ is computed as\n",
    "$$ W_{k+1}=W_k - \\frac{s_k y_k^T W_k+W_k y_k s_k^T}{y_k^T s_k} +\\left[1+\\frac{y_k^T W_k y_k}{y_k^T s_k}\\right]\\frac{s_k s_k^T}{y_k^T s_k} $$\n",
    "with $s_k=x_{k+1}-x_{k}$ and $y_k=\\nabla f(x_{k+1}) - \\nabla f(x_{k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def bfgs(f , f_grad , x0 , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    n = x0.size\n",
    "    g =  f_grad(x0)\n",
    "    sim_eval = 1\n",
    "    stop = PREC*np.linalg.norm( g )\n",
    "    \n",
    "    W = np.eye(n)\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n BFGS\\n------------------------------------\\nSTART\")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX): \n",
    "        g = f_grad(x)\n",
    "        gamma = line_search(f, f_grad, x, -g, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)[0]\n",
    "        x_plus = x - gamma*np.dot(W,g)\n",
    "        W = \n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
